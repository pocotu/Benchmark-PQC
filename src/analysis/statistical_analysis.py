#!/usr/bin/env python3
"""
Statistical Analysis for PQC Benchmarks

This script performs:
- An√°lisis estad√≠stico descriptivo
- Pruebas de hip√≥tesis (t-test, Mann-Whitney)
- C√°lculo de ratios de rendimiento
- Generaci√≥n de tablas LaTeX

Ubicaci√≥n: src/analysis/statistical_analysis.py
Salida: results/analysis/
"""

import pandas as pd
import numpy as np
from scipy import stats
from pathlib import Path
import json
from datetime import datetime

# Configuraci√≥n de rutas (relativas a la ra√≠z del proyecto)
BASE_DIR = Path(__file__).parent.parent.parent  # Benchmarks-PQC/
DATA_DIR = BASE_DIR / "data" / "processed"
OUTPUT_DIR = BASE_DIR / "results" / "analysis"

# Crear directorios si no existen
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_data():
    """Carga los datos procesados."""
    csv_path = DATA_DIR / "processed_data.csv"
    if not csv_path.exists():
        raise FileNotFoundError(f"No se encontr√≥ {csv_path}")
    
    df = pd.read_csv(csv_path)
    print(f"‚úì Datos cargados: {len(df)} registros")
    return df


def descriptive_statistics(df):
    """Genera estad√≠sticas descriptivas por arquitectura/algoritmo/operaci√≥n."""
    print("\n" + "="*60)
    print("AN√ÅLISIS ESTAD√çSTICO DESCRIPTIVO")
    print("="*60)
    
    # Resumen por arquitectura
    arch_summary = df.groupby('architecture').agg({
        'mean_us': ['mean', 'std', 'min', 'max'],
        'num_samples': 'sum'
    }).round(2)
    
    print("\nüìä Resumen por Arquitectura:")
    print(arch_summary)
    
    # Resumen por algoritmo
    algo_summary = df.groupby(['architecture', 'algorithm']).agg({
        'mean_us': 'mean',
        'stddev_us': 'mean',
        'cv_percent': 'mean'
    }).round(2)
    
    print("\nüìä Resumen por Algoritmo:")
    print(algo_summary)
    
    return arch_summary, algo_summary


def calculate_ratios(df):
    """Calcula ratios de rendimiento ARM64/RISC-V64 y overhead QEMU."""
    print("\n" + "="*60)
    print("C√ÅLCULO DE RATIOS DE RENDIMIENTO")
    print("="*60)
    
    results = []
    
    # Obtener datos por arquitectura
    arm64 = df[df['architecture'] == 'ARM64'].set_index(['algorithm', 'operation'])
    riscv64 = df[df['architecture'] == 'RISC-V64'].set_index(['algorithm', 'operation'])
    x86_64 = df[df['architecture'] == 'x86_64'].set_index(['algorithm', 'operation'])
    
    for idx in arm64.index:
        if idx in riscv64.index and idx in x86_64.index:
            arm_time = arm64.loc[idx, 'mean_us']
            riscv_time = riscv64.loc[idx, 'mean_us']
            x86_time = x86_64.loc[idx, 'mean_us']
            
            # Ratio ARM64/RISC-V64 (< 1 significa ARM64 m√°s r√°pido)
            ratio_arm_riscv = arm_time / riscv_time
            
            # Overhead QEMU vs x86_64
            overhead_arm = arm_time / x86_time
            overhead_riscv = riscv_time / x86_time
            
            # Ventaja porcentual de ARM64 sobre RISC-V64
            advantage_arm = ((riscv_time - arm_time) / riscv_time) * 100
            
            results.append({
                'algorithm': idx[0],
                'operation': idx[1],
                'arm64_us': arm_time,
                'riscv64_us': riscv_time,
                'x86_64_us': x86_time,
                'ratio_arm_riscv': ratio_arm_riscv,
                'overhead_arm_qemu': overhead_arm,
                'overhead_riscv_qemu': overhead_riscv,
                'advantage_arm_percent': advantage_arm
            })
    
    ratios_df = pd.DataFrame(results)
    
    # Mostrar resultados
    print("\nüìä Ratios ARM64/RISC-V64:")
    for _, row in ratios_df.iterrows():
        faster = "ARM64" if row['advantage_arm_percent'] > 0 else "RISC-V64"
        diff = abs(row['advantage_arm_percent'])
        print(f"  {row['algorithm']:12} {row['operation']:8}: {faster} {diff:.1f}% m√°s r√°pido")
    
    # Resumen por tipo de algoritmo
    print("\nüìä Resumen por Tipo de Algoritmo:")
    mlkem = ratios_df[ratios_df['algorithm'].str.contains('ML-KEM')]
    mldsa = ratios_df[ratios_df['algorithm'].str.contains('ML-DSA')]
    
    print(f"  ML-KEM promedio: ARM64 {mlkem['advantage_arm_percent'].mean():.1f}% {'m√°s r√°pido' if mlkem['advantage_arm_percent'].mean() > 0 else 'm√°s lento'}")
    print(f"  ML-DSA promedio: ARM64 {mldsa['advantage_arm_percent'].mean():.1f}% {'m√°s r√°pido' if mldsa['advantage_arm_percent'].mean() > 0 else 'm√°s lento'}")
    
    # Guardar resultados
    ratios_df.to_csv(OUTPUT_DIR / "performance_ratios.csv", index=False)
    print(f"\n‚úì Ratios guardados en {OUTPUT_DIR / 'performance_ratios.csv'}")
    
    return ratios_df


def hypothesis_tests(df):
    """Realiza pruebas de hip√≥tesis para comparar arquitecturas."""
    print("\n" + "="*60)
    print("PRUEBAS DE HIP√ìTESIS")
    print("="*60)
    
    results = []
    
    # Comparar ARM64 vs RISC-V64 para cada algoritmo/operaci√≥n
    algorithms = df['algorithm'].unique()
    operations = df['operation'].unique()
    
    for algo in algorithms:
        for op in operations:
            arm_data = df[(df['architecture'] == 'ARM64') & 
                         (df['algorithm'] == algo) & 
                         (df['operation'] == op)]
            riscv_data = df[(df['architecture'] == 'RISC-V64') & 
                           (df['algorithm'] == algo) & 
                           (df['operation'] == op)]
            
            if len(arm_data) == 0 or len(riscv_data) == 0:
                continue
            
            arm_mean = arm_data['mean_us'].values[0]
            riscv_mean = riscv_data['mean_us'].values[0]
            arm_std = arm_data['stddev_us'].values[0]
            riscv_std = riscv_data['stddev_us'].values[0]
            arm_n = arm_data['num_samples'].values[0]
            riscv_n = riscv_data['num_samples'].values[0]
            
            # Test t de Welch (no asume varianzas iguales)
            # Usando estad√≠stico t calculado manualmente
            se = np.sqrt((arm_std**2 / arm_n) + (riscv_std**2 / riscv_n))
            t_stat = (arm_mean - riscv_mean) / se if se > 0 else 0
            
            # Grados de libertad de Welch-Satterthwaite
            df_num = ((arm_std**2/arm_n) + (riscv_std**2/riscv_n))**2
            df_den = ((arm_std**2/arm_n)**2/(arm_n-1)) + ((riscv_std**2/riscv_n)**2/(riscv_n-1))
            dof = df_num / df_den if df_den > 0 else 1
            
            # p-value (two-tailed)
            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), dof))
            
            # Intervalo de confianza 95%
            t_crit = stats.t.ppf(0.975, dof)
            ci_lower = (arm_mean - riscv_mean) - t_crit * se
            ci_upper = (arm_mean - riscv_mean) + t_crit * se
            
            # Tama√±o del efecto (Cohen's d)
            pooled_std = np.sqrt(((arm_n-1)*arm_std**2 + (riscv_n-1)*riscv_std**2) / (arm_n + riscv_n - 2))
            cohens_d = (arm_mean - riscv_mean) / pooled_std if pooled_std > 0 else 0
            
            # Interpretaci√≥n
            significant = p_value < 0.05
            effect_size = "peque√±o" if abs(cohens_d) < 0.5 else "mediano" if abs(cohens_d) < 0.8 else "grande"
            
            results.append({
                'algorithm': algo,
                'operation': op,
                'arm64_mean': arm_mean,
                'riscv64_mean': riscv_mean,
                'difference': arm_mean - riscv_mean,
                't_statistic': t_stat,
                'p_value': p_value,
                'ci_95_lower': ci_lower,
                'ci_95_upper': ci_upper,
                'cohens_d': cohens_d,
                'significant': significant,
                'effect_size': effect_size
            })
    
    tests_df = pd.DataFrame(results)
    
    # Mostrar resultados
    print("\nüìä Resultados de Pruebas t de Welch (ARM64 vs RISC-V64):")
    print("-" * 80)
    for _, row in tests_df.iterrows():
        sig = "***" if row['p_value'] < 0.001 else "**" if row['p_value'] < 0.01 else "*" if row['p_value'] < 0.05 else ""
        print(f"  {row['algorithm']:12} {row['operation']:8}: t={row['t_statistic']:7.2f}, p={row['p_value']:.4f}{sig}, d={row['cohens_d']:.2f} ({row['effect_size']})")
    
    # Resumen
    significant_count = tests_df['significant'].sum()
    total_count = len(tests_df)
    print(f"\nüìä Resumen: {significant_count}/{total_count} comparaciones significativas (p < 0.05)")
    
    # Guardar resultados
    tests_df.to_csv(OUTPUT_DIR / "hypothesis_tests.csv", index=False)
    print(f"‚úì Pruebas guardadas en {OUTPUT_DIR / 'hypothesis_tests.csv'}")
    
    return tests_df


def generate_latex_tables(df, ratios_df):
    """Genera tablas LaTeX para la tesis."""
    print("\n" + "="*60)
    print("GENERACI√ìN DE TABLAS LATEX")
    print("="*60)
    
    # Tabla de ratios de rendimiento
    latex_ratios = """% Tabla de ratios de rendimiento - Generada autom√°ticamente
% Fecha: """ + datetime.now().strftime("%Y-%m-%d %H:%M") + """

\\begin{table}[H]
\\centering
\\caption{Ratios de rendimiento y overhead de emulaci√≥n QEMU}
\\label{tab:performance_ratios}
\\begin{tabular}{llrrrrr}
\\toprule
\\textbf{Algoritmo} & \\textbf{Op.} & \\textbf{ARM64} & \\textbf{RISC-V64} & \\textbf{Ratio} & \\textbf{Overhead} & \\textbf{Ventaja} \\\\
 & & ($\\mu$s) & ($\\mu$s) & ARM/RV & QEMU & ARM64 \\\\
\\midrule
"""
    
    for _, row in ratios_df.iterrows():
        latex_ratios += f"{row['algorithm']} & {row['operation']} & {row['arm64_us']:.2f} & {row['riscv64_us']:.2f} & {row['ratio_arm_riscv']:.2f} & {row['overhead_arm_qemu']:.1f}√ó & {row['advantage_arm_percent']:.1f}\\% \\\\\n"
    
    latex_ratios += """\\bottomrule
\\end{tabular}
\\end{table}
"""
    
    # Guardar tabla LaTeX
    latex_path = OUTPUT_DIR / "tabla_ratios_rendimiento.tex"
    with open(latex_path, 'w') as f:
        f.write(latex_ratios)
    print(f"‚úì Tabla LaTeX guardada en {latex_path}")
    
    return latex_ratios


def analyze_performance_factors(df):
    """Analiza factores que afectan el rendimiento."""
    print("\n" + "="*60)
    print("AN√ÅLISIS DE FACTORES DE RENDIMIENTO")
    print("="*60)
    
    # Factor 1: Impacto del nivel de seguridad
    print("\nüìä Impacto del Nivel de Seguridad:")
    
    for arch in ['x86_64', 'ARM64', 'RISC-V64']:
        print(f"\n  {arch}:")
        arch_data = df[df['architecture'] == arch]
        
        # ML-KEM
        mlkem_data = arch_data[arch_data['algorithm'].str.contains('ML-KEM')]
        if len(mlkem_data) > 0:
            mlkem_512 = mlkem_data[mlkem_data['algorithm'] == 'ML-KEM-512']['mean_us'].mean()
            mlkem_768 = mlkem_data[mlkem_data['algorithm'] == 'ML-KEM-768']['mean_us'].mean()
            mlkem_1024 = mlkem_data[mlkem_data['algorithm'] == 'ML-KEM-1024']['mean_us'].mean()
            print(f"    ML-KEM: 512‚Üí768 = {mlkem_768/mlkem_512:.2f}√ó, 512‚Üí1024 = {mlkem_1024/mlkem_512:.2f}√ó")
        
        # ML-DSA
        mldsa_data = arch_data[arch_data['algorithm'].str.contains('ML-DSA')]
        if len(mldsa_data) > 0:
            mldsa_44 = mldsa_data[mldsa_data['algorithm'] == 'ML-DSA-44']['mean_us'].mean()
            mldsa_65 = mldsa_data[mldsa_data['algorithm'] == 'ML-DSA-65']['mean_us'].mean()
            mldsa_87 = mldsa_data[mldsa_data['algorithm'] == 'ML-DSA-87']['mean_us'].mean()
            print(f"    ML-DSA: 44‚Üí65 = {mldsa_65/mldsa_44:.2f}√ó, 44‚Üí87 = {mldsa_87/mldsa_44:.2f}√ó")
    
    # Factor 2: Comparaci√≥n de operaciones
    print("\nüìä Costo Relativo de Operaciones (vs KeyGen):")
    
    for arch in ['x86_64', 'ARM64', 'RISC-V64']:
        print(f"\n  {arch}:")
        arch_data = df[df['architecture'] == arch]
        
        # ML-KEM
        for algo in ['ML-KEM-512', 'ML-KEM-768', 'ML-KEM-1024']:
            algo_data = arch_data[arch_data['algorithm'] == algo]
            if len(algo_data) >= 3:
                keygen = algo_data[algo_data['operation'] == 'keygen']['mean_us'].values[0]
                encaps = algo_data[algo_data['operation'] == 'encaps']['mean_us'].values[0]
                decaps = algo_data[algo_data['operation'] == 'decaps']['mean_us'].values[0]
                print(f"    {algo}: Encaps={encaps/keygen:.2f}√ó, Decaps={decaps/keygen:.2f}√ó")
        
        # ML-DSA
        for algo in ['ML-DSA-44', 'ML-DSA-65', 'ML-DSA-87']:
            algo_data = arch_data[arch_data['algorithm'] == algo]
            if len(algo_data) >= 3:
                keygen = algo_data[algo_data['operation'] == 'keygen']['mean_us'].values[0]
                sign = algo_data[algo_data['operation'] == 'sign']['mean_us'].values[0]
                verify = algo_data[algo_data['operation'] == 'verify']['mean_us'].values[0]
                print(f"    {algo}: Sign={sign/keygen:.2f}√ó, Verify={verify/keygen:.2f}√ó")


def model_tls_overhead(df):
    """Modela el overhead de handshake TLS 1.3 con PQC."""
    print("\n" + "="*60)
    print("MODELADO DE OVERHEAD TLS 1.3")
    print("="*60)
    
    # Usar ML-KEM-768 y ML-DSA-65 (nivel NIST 3, recomendado)
    results = {}
    
    for arch in ['x86_64', 'ARM64', 'RISC-V64']:
        arch_data = df[df['architecture'] == arch]
        
        # ML-KEM-768
        mlkem = arch_data[arch_data['algorithm'] == 'ML-KEM-768']
        keygen_kem = mlkem[mlkem['operation'] == 'keygen']['mean_us'].values[0]
        encaps = mlkem[mlkem['operation'] == 'encaps']['mean_us'].values[0]
        decaps = mlkem[mlkem['operation'] == 'decaps']['mean_us'].values[0]
        
        # ML-DSA-65
        mldsa = arch_data[arch_data['algorithm'] == 'ML-DSA-65']
        sign = mldsa[mldsa['operation'] == 'sign']['mean_us'].values[0]
        verify = mldsa[mldsa['operation'] == 'verify']['mean_us'].values[0]
        
        # Overhead de handshake TLS 1.3
        # Cliente: Encaps + Verify (certificado servidor)
        client_overhead = encaps + verify
        # Servidor: KeyGen + Decaps + Sign (CertificateVerify)
        server_overhead = keygen_kem + decaps + sign
        total_overhead = client_overhead + server_overhead
        
        results[arch] = {
            'client_us': client_overhead,
            'server_us': server_overhead,
            'total_us': total_overhead,
            'total_ms': total_overhead / 1000
        }
        
        print(f"\n  {arch}:")
        print(f"    Cliente (Encaps + Verify): {client_overhead:.2f} ¬µs")
        print(f"    Servidor (KeyGen + Decaps + Sign): {server_overhead:.2f} ¬µs")
        print(f"    Total: {total_overhead:.2f} ¬µs ({total_overhead/1000:.3f} ms)")
    
    # Comparaci√≥n con TLS cl√°sico (estimado ~50-100 ¬µs)
    print("\nüìä Comparaci√≥n con TLS Cl√°sico (ECDH + ECDSA, ~75 ¬µs):")
    for arch, data in results.items():
        overhead_factor = data['total_us'] / 75
        print(f"    {arch}: {overhead_factor:.1f}√ó overhead vs cl√°sico")
    
    # Guardar resultados
    tls_df = pd.DataFrame(results).T
    tls_df.to_csv(OUTPUT_DIR / "tls_overhead_model.csv")
    print(f"\n‚úì Modelo TLS guardado en {OUTPUT_DIR / 'tls_overhead_model.csv'}")
    
    return results


def model_pki_throughput(df):
    """Modela el throughput de emisi√≥n de certificados PKI."""
    print("\n" + "="*60)
    print("MODELADO DE THROUGHPUT PKI")
    print("="*60)
    
    results = {}
    
    for arch in ['x86_64', 'ARM64', 'RISC-V64']:
        arch_data = df[df['architecture'] == arch]
        
        # ML-DSA-65 Sign (operaci√≥n cr√≠tica para emisi√≥n)
        mldsa = arch_data[arch_data['algorithm'] == 'ML-DSA-65']
        sign_time = mldsa[mldsa['operation'] == 'sign']['mean_us'].values[0]
        
        # Throughput te√≥rico (solo Sign)
        throughput_theoretical = 1_000_000 / sign_time  # certs/segundo
        
        # Throughput con overhead de I/O (10ms, 50ms)
        throughput_10ms = 1_000_000 / (sign_time + 10_000)
        throughput_50ms = 1_000_000 / (sign_time + 50_000)
        
        results[arch] = {
            'sign_time_us': sign_time,
            'throughput_theoretical': throughput_theoretical,
            'throughput_10ms_overhead': throughput_10ms,
            'throughput_50ms_overhead': throughput_50ms
        }
        
        print(f"\n  {arch}:")
        print(f"    Sign time: {sign_time:.2f} ¬µs")
        print(f"    Throughput te√≥rico: {throughput_theoretical:.0f} certs/s")
        print(f"    Throughput (10ms overhead): {throughput_10ms:.0f} certs/s")
        print(f"    Throughput (50ms overhead): {throughput_50ms:.0f} certs/s")
    
    # Guardar resultados
    pki_df = pd.DataFrame(results).T
    pki_df.to_csv(OUTPUT_DIR / "pki_throughput_model.csv")
    print(f"\n‚úì Modelo PKI guardado en {OUTPUT_DIR / 'pki_throughput_model.csv'}")
    
    return results


def generate_summary_report(df, ratios_df, tests_df):
    """Genera un reporte resumen en JSON."""
    print("\n" + "="*60)
    print("GENERACI√ìN DE REPORTE RESUMEN")
    print("="*60)
    
    # Calcular m√©tricas resumen
    mlkem_ratios = ratios_df[ratios_df['algorithm'].str.contains('ML-KEM')]
    mldsa_ratios = ratios_df[ratios_df['algorithm'].str.contains('ML-DSA')]
    
    report = {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'total_records': len(df),
            'architectures': list(df['architecture'].unique()),
            'algorithms': list(df['algorithm'].unique())
        },
        'summary': {
            'mlkem': {
                'arm64_advantage_percent': mlkem_ratios['advantage_arm_percent'].mean(),
                'avg_ratio_arm_riscv': mlkem_ratios['ratio_arm_riscv'].mean(),
                'avg_overhead_qemu': mlkem_ratios['overhead_arm_qemu'].mean()
            },
            'mldsa': {
                'arm64_advantage_percent': mldsa_ratios['advantage_arm_percent'].mean(),
                'avg_ratio_arm_riscv': mldsa_ratios['ratio_arm_riscv'].mean(),
                'avg_overhead_qemu': mldsa_ratios['overhead_arm_qemu'].mean()
            }
        },
        'hypothesis_tests': {
            'total_comparisons': len(tests_df),
            'significant_at_005': int(tests_df['significant'].sum()),
            'avg_effect_size': tests_df['cohens_d'].abs().mean()
        },
        'key_findings': [
            f"ML-KEM: ARM64 y RISC-V64 tienen rendimiento equivalente (diferencia {mlkem_ratios['advantage_arm_percent'].mean():.1f}%)",
            f"ML-DSA: ARM64 es {mldsa_ratios['advantage_arm_percent'].mean():.1f}% m√°s r√°pido que RISC-V64",
            f"Overhead QEMU promedio: {ratios_df['overhead_arm_qemu'].mean():.1f}√ó vs x86_64 nativo",
            f"Sign es la operaci√≥n m√°s costosa en ML-DSA (2-2.5√ó KeyGen)"
        ]
    }
    
    # Guardar reporte
    report_path = OUTPUT_DIR / "analysis_summary.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"‚úì Reporte guardado en {report_path}")
    
    # Mostrar hallazgos clave
    print("\nüìã HALLAZGOS CLAVE:")
    for finding in report['key_findings']:
        print(f"  ‚Ä¢ {finding}")
    
    return report


def main():
    """Funci√≥n principal de an√°lisis."""
    print("="*60)
    print("AN√ÅLISIS ESTAD√çSTICO - BENCHMARKS PQC")
    print("Tesis: ML-KEM y ML-DSA en ARM y RISC-V")
    print("="*60)
    print(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    # Cargar datos
    df = load_data()
    
    # An√°lisis estad√≠stico descriptivo
    arch_summary, algo_summary = descriptive_statistics(df)
    
    # Calcular ratios de rendimiento
    ratios_df = calculate_ratios(df)
    
    # Pruebas de hip√≥tesis
    tests_df = hypothesis_tests(df)
    
    # Generar tablas LaTeX
    generate_latex_tables(df, ratios_df)
    
    # An√°lisis de factores de rendimiento
    analyze_performance_factors(df)
    
    # Modelado de overhead TLS
    model_tls_overhead(df)
    
    # Modelado de throughput PKI
    model_pki_throughput(df)
    
    # Generar reporte resumen
    report = generate_summary_report(df, ratios_df, tests_df)
    
    print("\n" + "="*60)
    print("‚úÖ AN√ÅLISIS COMPLETADO")
    print("="*60)
    print(f"Archivos generados en: {OUTPUT_DIR}")
    
    return df, ratios_df, tests_df, report


if __name__ == "__main__":
    main()
